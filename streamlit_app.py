# -*- coding: utf-8 -*-
"""streamlit_app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k_B8qAPSoyk_X6w_1_NZVqEPQAQUR9Wx
"""
import nltk
nltk.data.path.append('./nltk_data')

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

import streamlit as st
import pandas as pd
import json
import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Define preprocessing
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text)
    tokens = word_tokenize(text)
    tokens = [t for t in tokens if t not in stop_words and len(t) > 1]
    tokens = [lemmatizer.lemmatize(t) for t in tokens]
    return tokens

# Scoring
skill_keywords = [
    "python", "java", "sql", "excel", "communication", "machine", "learning",
    "deep", "analysis", "project", "data", "management", "research"
]

def compute_score(resume_tokens, jd_tokens):
    matched_terms = set(jd_tokens) & set(resume_tokens)
    keyword_score = len(matched_terms) / len(set(jd_tokens)) * 70
    matched_skills = set(skill_keywords) & set(resume_tokens)
    skill_score = len(matched_skills) / len(skill_keywords) * 30
    return round(keyword_score + skill_score, 2)

def summarize_candidate(resume_tokens, jd_tokens):
    matched_terms = set(resume_tokens) & set(jd_tokens)
    matched_skills = set(resume_tokens) & set(skill_keywords)
    summary = {
        "Matched Skills": list(matched_skills),
        "Relevant JD Terms": list(matched_terms),
        "Insight": f"Matched {len(matched_terms)} JD terms and {len(matched_skills)} skill(s)"
    }
    return summary

# Streamlit App
st.title("Resume Shortlisting System")

# File uploader
uploaded_files = st.file_uploader("Upload Resume JSON Files", accept_multiple_files=True)
job_description = st.text_area("Enter Job Description")

if uploaded_files and job_description:
    # Preprocess JD
    jd_tokens = preprocess_text(job_description)

    results = []
    for f in uploaded_files:
        entry = json.load(f)
        resume_text = entry['input']['resume']
        resume_tokens = preprocess_text(resume_text)
        score = compute_score(resume_tokens, jd_tokens)
        summary = summarize_candidate(resume_tokens, jd_tokens)
        results.append({
            "Resume": resume_text[:300] + "...",
            "Score": score,
            "Summary": summary
        })

    # Show results
    top_k = st.slider("Top K Candidates", min_value=1, max_value=min(10, len(results)), value=5)
    sorted_results = sorted(results, key=lambda x: x["Score"], reverse=True)[:top_k]

    for i, res in enumerate(sorted_results):
        st.subheader(f"Candidate #{i+1}")
        st.write("Score:", res["Score"])
        st.write("Resume (Preview):", res["Resume"])
        st.write("Matched Skills:", ", ".join(res["Summary"]["Matched Skills"]))
        st.write("Relevant JD Terms:", ", ".join(res["Summary"]["Relevant JD Terms"]))
        st.write("Insight:", res["Summary"]["Insight"])
